{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":209883909,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import login\n\n# Paste your Hugging Face API token here\nHF_TOKEN = \"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\"\n\n# Log in using the token\nlogin(HF_TOKEN)\n\nimport wandb\n\n# Replace with your actual WandB API key\nwandb.login(key=\"a588c4489ec28754ba87e0209ef0299114ed0699\")\nimport os\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"jax\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:35:27.756007Z","iopub.execute_input":"2024-11-27T18:35:27.757017Z","iopub.status.idle":"2024-11-27T18:35:27.947346Z","shell.execute_reply.started":"2024-11-27T18:35:27.756976Z","shell.execute_reply":"2024-11-27T18:35:27.946544Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mibasit050802\u001b[0m (\u001b[33mibasit050802-national-university-of-computer-and-emergin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /kaggle/usr/lib/text_to_image_finetune_template_lora/wandb/ wasn't writable, using system temp directory\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /kaggle/usr/lib/text_to_image_finetune_template_lora/wandb/ wasn't writable, using system temp directory.\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install tensorboard\n!pip install accelerate>=0.22.0\n!pip install torchvision\n!pip install transformers>=4.25.1\n!pip install ftfy\n!pip install Jinja2\n!pip install datasets\n!pip install peft==0.7.0\n!pip uninstall -y diffusers\n!pip install git+https://github.com/huggingface/diffusers.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:24:41.749264Z","iopub.execute_input":"2024-11-27T18:24:41.749632Z","iopub.status.idle":"2024-11-27T18:26:13.013230Z","shell.execute_reply.started":"2024-11-27T18:24:41.749605Z","shell.execute_reply":"2024-11-27T18:26:13.012362Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.16.2)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.62.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.6)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (70.0.0)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.4.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchvision) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchvision) (1.3.0)\nCollecting ftfy\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.13)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.3.1\nRequirement already satisfied: Jinja2 in /opt/conda/lib/python3.10/site-packages (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2) (2.1.5)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting peft==0.7.0\n  Downloading peft-0.7.0-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.0) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.0) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.0) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.0) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.7.0) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.0) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.7.0) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.7.0) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.7.0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.0) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.7.0) (1.3.0)\nDownloading peft-0.7.0-py3-none-any.whl (168 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.7.0\n\u001b[33mWARNING: Skipping diffusers as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mCollecting git+https://github.com/huggingface/diffusers.git\n  Cloning https://github.com/huggingface/diffusers.git to /tmp/pip-req-build-ictys40y\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers.git /tmp/pip-req-build-ictys40y\n  Resolved https://github.com/huggingface/diffusers.git to commit 8d477daed507801a50dc9f285c982b1c8051ae2d\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: importlib_metadata in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (7.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (0.25.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (2.32.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (0.4.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers==0.32.0.dev0) (10.3.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (6.0.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (4.12.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib_metadata->diffusers==0.32.0.dev0) (3.19.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.32.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.32.0.dev0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.32.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers==0.32.0.dev0) (2024.8.30)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.23.2->diffusers==0.32.0.dev0) (3.1.2)\nBuilding wheels for collected packages: diffusers\n  Building wheel for diffusers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for diffusers: filename=diffusers-0.32.0.dev0-py3-none-any.whl size=2977364 sha256=4568cbe9a0d983cff5b15620a728fdf1521957210643066ee62a95e43d7cc3f5\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ax2l0_1h/wheels/4d/b7/a8/6f9549ceec5daad78675b857ac57d697c387062506520a7b50\nSuccessfully built diffusers\nInstalling collected packages: diffusers\nSuccessfully installed diffusers-0.32.0.dev0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\n\nos.chdir(\"/kaggle/usr/lib/text_to_image_finetune_template_lora\")\n\n!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:26:13.014552Z","iopub.execute_input":"2024-11-27T18:26:13.014940Z","iopub.status.idle":"2024-11-27T18:26:14.007317Z","shell.execute_reply.started":"2024-11-27T18:26:13.014902Z","shell.execute_reply":"2024-11-27T18:26:14.006228Z"}},"outputs":[{"name":"stdout","text":"/kaggle/usr/lib/text_to_image_finetune_template_lora\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n\n!accelerate launch text_to_image_finetune_template_lora.py \\\n  --mixed_precision=\"fp16\" \\\n  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2\" \\\n  --dataset_name=\"Ibbi02/genai-project-dataset-without-trigger-100\" \\\n  --output_dir=\"/kaggle/working/finetuned_sd_v1_4\" \\\n  --image_column=\"image\" \\\n  --caption_column=\"text\"\\\n  --validation_prompt=\"In the image, there is a living room with a hardwood floor. The room features a large white couch situated against the wall, and a dining table can be seen nearby. A chair is placed in front of the couch, and a potted plant is placed near the table.\" \\\n  --checkpointing_steps=718 \\\n  --num_validation_images=1 \\\n  --resolution=768 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=1000 \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=1 \\\n  --learning_rate=1e-4 \\\n  --max_grad_norm=1 \\\n  --gradient_checkpointing \\\n  --snr_gamma=3.0 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --seed=\"0\"Â \\\n  --rank=96 \\\n  --report_to=\"wandb\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T18:36:50.362354Z","iopub.execute_input":"2024-11-27T18:36:50.362710Z","iopub.status.idle":"2024-11-27T18:44:16.364834Z","shell.execute_reply.started":"2024-11-27T18:36:50.362676Z","shell.execute_reply":"2024-11-27T18:44:16.363944Z"}},"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nscheduler/scheduler_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 345/345 [00:00<00:00, 2.00MB/s]\n{'clip_sample_range', 'sample_max_value', 'variance_type', 'thresholding', 'timestep_spacing', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\ntokenizer/tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 824/824 [00:00<00:00, 5.88MB/s]\ntokenizer/vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.06M/1.06M [00:00<00:00, 16.3MB/s]\ntokenizer/merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 525k/525k [00:00<00:00, 17.4MB/s]\ntokenizer/special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460/460 [00:00<00:00, 3.25MB/s]\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\ntext_encoder/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 633/633 [00:00<00:00, 3.51MB/s]\nmodel.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36G/1.36G [00:06<00:00, 221MB/s]\nvae/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 611/611 [00:00<00:00, 5.08MB/s]\ndiffusion_pytorch_model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 335M/335M [00:01<00:00, 221MB/s]\n{'latents_std', 'shift_factor', 'use_quant_conv', 'mid_block_add_attention', 'use_post_quant_conv', 'scaling_factor', 'latents_mean', 'force_upcast'} was not found in config. Values will be initialized to default values.\nunet/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 909/909 [00:00<00:00, 6.24MB/s]\ndiffusion_pytorch_model.safetensors: 100%|â–ˆâ–ˆ| 3.46G/3.46G [00:16<00:00, 215MB/s]\n{'time_embedding_act_fn', 'resnet_skip_time_act', 'projection_class_embeddings_input_dim', 'dropout', 'mid_block_only_cross_attention', 'addition_time_embed_dim', 'reverse_transformer_layers_per_block', 'encoder_hid_dim', 'time_embedding_dim', 'time_embedding_type', 'conv_out_kernel', 'transformer_layers_per_block', 'class_embeddings_concat', 'upcast_attention', 'attention_type', 'resnet_out_scale_factor', 'cross_attention_norm', 'encoder_hid_dim_type', 'class_embed_type', 'addition_embed_type', 'conv_in_kernel', 'num_attention_heads', 'timestep_post_act', 'time_cond_proj_dim', 'addition_embed_type_num_heads', 'resnet_time_scale_shift', 'mid_block_type', 'num_class_embeds', 'only_cross_attention'} was not found in config. Values will be initialized to default values.\nREADME.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 310/310 [00:00<00:00, 2.03MB/s]\ntrain-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.38M/1.38M [00:00<00:00, 24.7MB/s]\nGenerating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1362.63 examples/s]\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /kaggle/usr/lib/text_to_image_finetune_template_lora/wandb/ wasn't writable, using system temp directory.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /kaggle/usr/lib/text_to_image_finetune_template_lora/wandb/ wasn't writable, using system temp directory\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mibasit050802\u001b[0m (\u001b[33mibasit050802-national-university-of-computer-and-emergin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.3\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/tmp/wandb/run-20241127_183737-wiiseckc\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdry-donkey-1\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ibasit050802-national-university-of-computer-and-emergin/text2image-fine-tune\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ibasit050802-national-university-of-computer-and-emergin/text2image-fine-tune/runs/wiiseckc\u001b[0m\nSteps:   0%|                                           | 0/1000 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\nSteps:   5%|â–Ž     | 50/1000 [00:53<16:32,  1.04s/it, lr=0.0001, step_loss=0.225]\nmodel_index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 537/537 [00:00<00:00, 2.28MB/s]\u001b[A\n\nFetching 11 files:   0%|                                 | 0/11 [00:00<?, ?it/s]\u001b[A\n\n(â€¦)ature_extractor/preprocessor_config.json: 100%|â–ˆ| 342/342 [00:00<00:00, 1.22M\u001b[A\u001b[A\n\nFetching 11 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 65.43it/s]\u001b[A\n{'image_encoder'} was not found in config. Values will be initialized to default values.\n\nLoading pipeline components...:   0%|                     | 0/6 [00:00<?, ?it/s]\u001b[A{'clip_sample_range', 'sample_max_value', 'thresholding', 'timestep_spacing', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\nLoaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2.\nLoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2.\nLoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2.\n\nLoading pipeline components...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 3/6 [00:01<00:01,  2.74it/s]\u001b[A/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nLoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2.\n{'latents_std', 'shift_factor', 'use_quant_conv', 'mid_block_add_attention', 'use_post_quant_conv', 'scaling_factor', 'latents_mean', 'force_upcast'} was not found in config. Values will be initialized to default values.\nLoaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2.\n\nLoading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  4.21it/s]\u001b[A\nSteps:  10%|â–   | 100/1000 [02:04<17:06,  1.14s/it, lr=0.0001, step_loss=0.0242]{'image_encoder'} was not found in config. Values will be initialized to default values.\n\nLoading pipeline components...:   0%|                     | 0/6 [00:00<?, ?it/s]\u001b[A{'clip_sample_range', 'sample_max_value', 'thresholding', 'timestep_spacing', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\nLoaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2.\nLoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2.\nLoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2.\n\nLoading pipeline components...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 3/6 [00:00<00:00,  3.76it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2.\n{'latents_std', 'shift_factor', 'use_quant_conv', 'mid_block_add_attention', 'use_post_quant_conv', 'scaling_factor', 'latents_mean', 'force_upcast'} was not found in config. Values will be initialized to default values.\nLoaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2.\n\nLoading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  5.15it/s]\u001b[A\nSteps:  15%|â–Œ   | 150/1000 [03:16<15:38,  1.10s/it, lr=0.0001, step_loss=0.0416]{'image_encoder'} was not found in config. Values will be initialized to default values.\n\nLoading pipeline components...:   0%|                     | 0/6 [00:00<?, ?it/s]\u001b[A{'clip_sample_range', 'sample_max_value', 'thresholding', 'timestep_spacing', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\nLoaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2.\nLoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2.\nLoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2.\n\nLoading pipeline components...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 3/6 [00:00<00:00,  3.78it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2.\n{'latents_std', 'shift_factor', 'use_quant_conv', 'mid_block_add_attention', 'use_post_quant_conv', 'scaling_factor', 'latents_mean', 'force_upcast'} was not found in config. Values will be initialized to default values.\nLoaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2.\n\nLoading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  5.39it/s]\u001b[A\nSteps:  20%|â–Š   | 200/1000 [04:28<14:39,  1.10s/it, lr=0.0001, step_loss=0.0291]{'image_encoder'} was not found in config. Values will be initialized to default values.\n\nLoading pipeline components...:   0%|                     | 0/6 [00:00<?, ?it/s]\u001b[A{'clip_sample_range', 'sample_max_value', 'thresholding', 'timestep_spacing', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\nLoaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2.\nLoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2.\nLoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2.\n\nLoading pipeline components...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 3/6 [00:00<00:00,  3.75it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2.\n{'latents_std', 'shift_factor', 'use_quant_conv', 'mid_block_add_attention', 'use_post_quant_conv', 'scaling_factor', 'latents_mean', 'force_upcast'} was not found in config. Values will be initialized to default values.\nLoaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2.\n\nLoading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  5.38it/s]\u001b[A\nSteps:  25%|â–ˆâ–Œ    | 250/1000 [05:39<13:49,  1.11s/it, lr=0.0001, step_loss=0.14]{'image_encoder'} was not found in config. Values will be initialized to default values.\n\nLoading pipeline components...:   0%|                     | 0/6 [00:00<?, ?it/s]\u001b[A{'clip_sample_range', 'sample_max_value', 'thresholding', 'timestep_spacing', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\nLoaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2.\nLoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2.\nLoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2.\n\nLoading pipeline components...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 3/6 [00:01<00:01,  2.95it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2.\n{'latents_std', 'shift_factor', 'use_quant_conv', 'mid_block_add_attention', 'use_post_quant_conv', 'scaling_factor', 'latents_mean', 'force_upcast'} was not found in config. Values will be initialized to default values.\nLoaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2.\n\nLoading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  4.51it/s]\u001b[A\nSteps:  28%|â–ˆâ–  | 285/1000 [06:34<13:39,  1.15s/it, lr=0.0001, step_loss=0.0393]^C\nW1127 18:44:15.659000 131954682435392 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGINT death signal, shutting down workers\nW1127 18:44:15.660000 131954682435392 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 427 closing signal SIGINT\nW1127 18:44:15.660000 131954682435392 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 428 closing signal SIGINT\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/kaggle/usr/lib/text_to_image_finetune_template_lora/text_to_image_finetune_template_lora.py\", line 980, in <module>\n[rank1]:     main()\n[rank1]:   File \"/kaggle/usr/lib/text_to_image_finetune_template_lora/text_to_image_finetune_template_lora.py\", line 858, in main\n[rank1]:     accelerator.backward(loss)\n[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2192, in backward\n[rank1]:     self.scaler.scale(loss).backward(**kwargs)\n[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 521, in backward\n[rank1]:     torch.autograd.backward(\n[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 289, in backward\n[rank1]:     _engine_run_backward(\n[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py\", line 768, in _engine_run_backward\n[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n[rank1]: KeyboardInterrupt\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/kaggle/usr/lib/text_to_image_finetune_template_lora/text_to_image_finetune_template_lora.py\", line 980, in <module>\n[rank0]:     main()\n[rank0]:   File \"/kaggle/usr/lib/text_to_image_finetune_template_lora/text_to_image_finetune_template_lora.py\", line 858, in main\n[rank0]:     accelerator.backward(loss)\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2192, in backward\n[rank0]:     self.scaler.scale(loss).backward(**kwargs)\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 521, in backward\n[rank0]:     torch.autograd.backward(\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 289, in backward\n[rank0]:     _engine_run_backward(\n[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py\", line 768, in _engine_run_backward\n[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n[rank0]: KeyboardInterrupt\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}